{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zqrc0\\AppData\\Local\\Temp\\ipykernel_9108\\1095212270.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Checkpoint loaded from model/07/best_by_val_loss_ep67.pth (epoch=66)\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_000.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_001.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_002.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_003.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_004.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_005.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_006.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_007.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_008.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_009.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_010.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_011.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_012.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_013.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_014.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_015.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_016.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_017.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_018.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_019.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_020.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_021.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_022.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_023.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_024.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_025.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_026.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_027.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_028.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_029.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_030.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_031.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_032.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_033.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_034.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_035.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_036.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_037.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_038.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_039.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_040.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_041.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_042.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_043.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_044.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_045.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_046.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_047.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_048.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_049.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_050.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_051.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_052.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_053.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_054.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_055.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_056.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_057.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_058.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_059.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_060.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_061.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_062.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_063.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_064.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_065.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_066.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_067.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_068.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_069.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_070.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_071.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_072.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_073.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_074.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_075.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_076.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_077.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_078.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_079.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_080.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_081.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_082.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_083.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_084.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_085.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_086.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_087.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_088.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_089.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_090.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_091.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_092.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_093.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_094.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_095.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_096.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_097.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_098.png\n",
      "Saved: data/output_grayTocol_2025010602\\TEST_099.png\n"
     ]
    }
   ],
   "source": [
    "# test.py\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# -----------------------\n",
    "# (1) 학습 때 쓰인 함수/클래스 재정의 (또는 from train import ...)\n",
    "# -----------------------\n",
    "\n",
    "def lab_to_rgb(L, a, b):\n",
    "    lab_0_255 = np.zeros((L.shape[0], L.shape[1], 3), dtype=np.float32)\n",
    "    lab_0_255[:,:,0] = L * 255.0\n",
    "    lab_0_255[:,:,1] = a * 128.0 + 128.0\n",
    "    lab_0_255[:,:,2] = b * 128.0 + 128.0\n",
    "    lab_0_255 = np.clip(lab_0_255, 0, 255).astype(np.uint8)\n",
    "    bgr = cv2.cvtColor(lab_0_255, cv2.COLOR_Lab2BGR)\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    rgb = np.clip(rgb, 0, 255).astype(np.uint8)\n",
    "    return rgb / 255.0\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        net = models.resnet50(weights=ResNet50_Weights.DEFAULT if pretrained else None)\n",
    "        self.initial = nn.Sequential(net.conv1, net.bn1, net.relu)\n",
    "        self.maxpool = net.maxpool\n",
    "        self.layer1 = net.layer1\n",
    "        self.layer2 = net.layer2\n",
    "        self.layer3 = net.layer3\n",
    "        self.layer4 = net.layer4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.initial(x)\n",
    "        x1 = self.maxpool(x0)\n",
    "        x1 = self.layer1(x1)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "        return x0, x1, x2, x3, x4\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, out_ch=2, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNetEncoder(pretrained=pretrained)\n",
    "        self.up3 = UpConv(2048, 1024)\n",
    "        self.dec3 = DoubleConv(2048, 1024)\n",
    "        self.up2 = UpConv(1024, 512)\n",
    "        self.dec2 = DoubleConv(1024, 512)\n",
    "        self.up1 = UpConv(512, 256)\n",
    "        self.dec1 = DoubleConv(512, 256)\n",
    "        self.up0 = UpConv(256, 64)\n",
    "        self.dec0 = DoubleConv(128, 64)\n",
    "        self.up_final = UpConv(64,64)\n",
    "        self.dec_final = DoubleConv(64,64)\n",
    "        self.final_out = nn.Conv2d(64, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.repeat(1,3,1,1)\n",
    "        x0, x1, x2, x3, x4 = self.encoder(x)\n",
    "        x_up3 = self.up3(x4)\n",
    "        x_cat3 = torch.cat([x_up3, x3], dim=1)\n",
    "        x_dec3 = self.dec3(x_cat3)\n",
    "\n",
    "        x_up2 = self.up2(x_dec3)\n",
    "        x_cat2 = torch.cat([x_up2, x2], dim=1)\n",
    "        x_dec2 = self.dec2(x_cat2)\n",
    "\n",
    "        x_up1 = self.up1(x_dec2)\n",
    "        x_cat1 = torch.cat([x_up1, x1], dim=1)\n",
    "        x_dec1 = self.dec1(x_cat1)\n",
    "\n",
    "        x_up0 = self.up0(x_dec1)\n",
    "        x_cat0 = torch.cat([x_up0, x0], dim=1)\n",
    "        x_dec0 = self.dec0(x_cat0)\n",
    "\n",
    "        x_upf = self.up_final(x_dec0)\n",
    "        x_decf = self.dec_final(x_upf)\n",
    "        out = self.final_out(x_decf)\n",
    "        return out\n",
    "\n",
    "# -----------------------\n",
    "# (2) 체크포인트 로드 함수\n",
    "# -----------------------\n",
    "def load_checkpoint(checkpoint_path, model, map_location=\"cpu\"):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    print(f\"[Test] Checkpoint loaded from {checkpoint_path} (epoch={checkpoint['epoch']})\")\n",
    "\n",
    "# -----------------------\n",
    "# (3) 테스트 로직\n",
    "# -----------------------\n",
    "def test_model(model_path, \n",
    "               test_gray_dir, \n",
    "               test_mask_dir, \n",
    "               output_dir,\n",
    "               device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "               img_ext=\".png\"):\n",
    "\n",
    "    # 1) 모델 생성 & 체크포인트 로드\n",
    "    model = ResNetUNet(out_ch=2, pretrained=False).to(device)\n",
    "    load_checkpoint(model_path, model, map_location=device)\n",
    "    model.eval()\n",
    "\n",
    "    # 2) 변환 정의\n",
    "    transform_gray = transforms.Compose([\n",
    "        transforms.Resize((512,512)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 3) 테스트 파일 목록\n",
    "    gray_image_paths = sorted(glob.glob(os.path.join(test_gray_dir, f\"*{img_ext}\")))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for gray_path in gray_image_paths:\n",
    "            fname = os.path.basename(gray_path)\n",
    "            mask_path = os.path.join(test_mask_dir, fname)\n",
    "\n",
    "            if not os.path.exists(mask_path):\n",
    "                print(f\"No matching mask found for {fname}, skipping...\")\n",
    "                continue\n",
    "\n",
    "            # 4) 흑백 이미지 변환\n",
    "            gray_img = Image.open(gray_path).convert('L')\n",
    "            gray_tensor = transform_gray(gray_img)  # [1,H,W]\n",
    "\n",
    "            # 마스크 로드\n",
    "            mask_img = Image.open(mask_path).convert('L')\n",
    "            mask_np = np.array(mask_img)\n",
    "            mask_bin = (mask_np > 128).astype(np.float32)\n",
    "            mask_bin = torch.from_numpy(mask_bin).unsqueeze(0)  # [1,H,W]\n",
    "\n",
    "            # 5) 모델 추론\n",
    "            gray_tensor = gray_tensor.unsqueeze(0).to(device)  # [1,1,H,W]\n",
    "            pred_ab = model(gray_tensor)                      # [1,2,H,W]\n",
    "\n",
    "            # 6) Lab -> RGB\n",
    "            pred_ab_np = pred_ab[0].cpu().permute(1,2,0).numpy()  # [H,W,2]\n",
    "            L_np = gray_tensor[0,0].cpu().numpy()\n",
    "            pred_rgb = lab_to_rgb(L_np, pred_ab_np[:,:,0], pred_ab_np[:,:,1])\n",
    "\n",
    "            # 7) 결과 저장\n",
    "            out_path = os.path.join(output_dir, fname)\n",
    "            out_img = (pred_rgb*255).astype(np.uint8)\n",
    "            Image.fromarray(out_img).save(out_path)\n",
    "            print(f\"Saved: {out_path}\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# (4) 메인 진입점\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model_ckpt = \"model/07/best_by_val_loss_ep67.pth\" \n",
    "    test_gray_dir = \"../data/test_input\"\n",
    "    test_mask_dir = \"../data/output_01_mask\"\n",
    "    output_dir = \"data/output_grayTocol_2025010602\"\n",
    "\n",
    "    # 실행\n",
    "    test_model(\n",
    "        model_path=model_ckpt,\n",
    "        test_gray_dir=test_gray_dir,\n",
    "        test_mask_dir=test_mask_dir,\n",
    "        output_dir=output_dir\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagepro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
